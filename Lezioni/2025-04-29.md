# MartedÃ¬ 29 aprile 2025

Questa immagine mostra unâ€™**estensione dellâ€™algoritmo di discesa del gradiente** per un modello di **regressione lineare (o logistica)** con **termine di regolarizzazione L2 (Ridge/Lasso)**. Ti spiego passo passo cosa c'Ã¨ scritto e cosa significa.

---

## ðŸ§  Obiettivo:

Nel gradient descent standard, aggiorni i pesi $\theta_j$ per minimizzare una funzione di costo.
Quando aggiungi la **regolarizzazione**, vuoi anche **penalizzare i pesi grandi** per evitare overfitting.

---

## âœï¸ Cosa dice il testo:

> **"Nell'algoritmo di discesa del gradiente si aggiunge la derivata del termine di regolarizzazione per tutti i $v_j$ con $j \neq 0$"**

ðŸ‘‰ Questo significa:

* Il termine di **regolarizzazione** si applica **solo ai pesi** $v_1, ..., v_d$ e **non al bias** $v_0$ (o $\theta_0$), che **non va penalizzato**.

---

## ðŸ” Struttura dellâ€™algoritmo:

Per ogni iterazione del **gradient descent** si aggiornano i coefficienti cosÃ¬:

### ðŸŸ© **Bias (j = 0):**

$$
v_0^{\text{new}} \leftarrow v_0^{\text{old}} - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \left( h_v(x^{(i)}) - y^{(i)} \right)x_0
$$

* Questo Ã¨ lo **standard update** per il bias (nessuna regolarizzazione).
* Di solito $x_0 = 1$.

---

### ðŸŸ© **Altri pesi (j â‰  0):**

$$
v_j^{\text{new}} \leftarrow v_j^{\text{old}} - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} \left( h_v(x^{(i)}) - y^{(i)} \right)x_j + \frac{\lambda}{m} v_j \right]
$$

* C'Ã¨ il **termine classico del gradiente** (primo termine della somma).
* E in piÃ¹ un **termine di regolarizzazione** $\frac{\lambda}{m} v_j$ che tende a "restringere" i pesi verso zero.
* $\lambda$ Ã¨ il coefficiente di regolarizzazione (piÃ¹ Ã¨ grande, piÃ¹ i pesi vengono penalizzati).

---

## ðŸ“¦ Funzione costo (non mostrata ma implicita):

Questa derivata deriva da una **loss con regolarizzazione**:

$$
J(v) = \frac{1}{m} \sum_{i=1}^{m} \left( h_v(x^{(i)}) - y^{(i)} \right) + \frac{\lambda}{2m} \sum_{j=1}^{d} v_j^2
$$

* Il secondo termine Ã¨ la **penalitÃ ** (regolarizzazione L2).
* La derivata rispetto a $v_j$ include $+ \frac{\lambda}{m} v_j$ (da cui il termine nell'aggiornamento).

---

## ðŸ’¡ Conclusione:

Questo Ã¨ un algoritmo di **gradient descent con regolarizzazione L2** (tipica della **Ridge Regression** o **logistic regression regolarizzata**).

* âœ… $v_0$: aggiornato normalmente (senza penalizzazione).
* âœ… $v_j$, con $j \neq 0$: aggiornato con penalizzazione.
* âœ… Serve a **limitare lâ€™overfitting** penalizzando i pesi grandi.
* âœ… La regolarizzazione Ã¨ controllata da $\lambda$.

Questa immagine spiega in modo informale e pratico il concetto di **Stochastic Gradient Descent (SGD)** e il problema dell'efficienza computazionale nel Gradient Descent classico, soprattutto con dataset di grandi dimensioni. Ecco una spiegazione dettagliata di tutto il contenuto:

---

## ðŸ§  **Stochastic Gradient Descent (SGD)**

### ðŸ“Œ **Osservazioni iniziali**

> â€œLa discesa del gradiente completa (GD) usa tutti i campioni del training set.â€

âž¡ï¸ Nel **Gradient Descent classico**, ad **ogni iterazione** bisogna calcolare il **gradiente della funzione di costo su tutti i dati**.
Questo diventa **computazionalmente costoso** per dataset molto grandi.

---

### ðŸ“‰ **Problema di scalabilitÃ **

> â€œLa somma dei residui coinvolge m termini ad ogni iterazione.â€

* Se hai $m$ campioni, ad ogni passo devi sommare $m$ termini per calcolare il gradiente.

#### Esempio pratico:

* $m = 100.000$ campioni
* $100$ iterazioni â†’ ogni iterazione deve analizzare tutti i 100.000
  âž¡ï¸ In totale:

$$
m \times \text{iterazioni} = 100.000 \times 100 = 10.000.000 \quad \text{osservazioni processate}
$$

Se invece $m = 1.000.000$ campioni:

$$
100 \times 1.000.000 = 100.000.000 \quad \text{osservazioni!}
$$

### âš ï¸ Questo Ã¨ molto lento!

---

## âš¡ **Soluzione: Stochastic Gradient Descent**

> ðŸ¦¸â€â™‚ï¸ **â€œStochastic is super hero!â€**

* Invece di usare **tutti** i campioni ad ogni iterazione, ne **campiona uno o pochi** (mini-batch) per calcolare un'approssimazione del gradiente.

### âœ³ï¸ Procedura:

#### 1. **Campiona random** un sottoinsieme dei dati

â€“ Un piccolo **batch** di punti del training set.

#### 2. **Esegui una singola iterazione** di gradient descent

â€“ Calcola il gradiente **solo** su quei punti campionati.

#### 3. **Aggiorna i pesi** e **ripeti** finchÃ© non convergi

â€“ Continua finchÃ© non arrivi a una minima variazione nella funzione di costo.

> Questo metodo Ã¨ piÃ¹ **veloce**, anche se meno preciso a ogni iterazione â†’ ma converge bene nel lungo termine.


