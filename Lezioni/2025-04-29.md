# Marted√¨ 29 aprile 2025

## üß† Obiettivo:

Nel gradient descent standard, aggiorni i pesi $\theta_j$ per minimizzare una funzione di costo.
Quando aggiungi la **regolarizzazione**, vuoi anche **penalizzare i pesi grandi** per evitare overfitting.

---


> **"Nell'algoritmo di discesa del gradiente si aggiunge la derivata del termine di regolarizzazione per tutti i $v_j$ con $j \neq 0$"**

üëâ Questo significa:

* Il termine di **regolarizzazione** si applica **solo ai pesi** $v_1, ..., v_d$ e **non al bias** $v_0$ (o $\theta_0$), che **non va penalizzato**.

---

## üîÅ Struttura dell‚Äôalgoritmo:

Per ogni iterazione del **gradient descent** si aggiornano i coefficienti cos√¨:

### üü© **Bias (j = 0):**

$$
v_0^{\text{new}} \leftarrow v_0^{\text{old}} - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \left( h_v(x^{(i)}) - y^{(i)} \right)x_0
$$

* Questo √® lo **standard update** per il bias (nessuna regolarizzazione).
* Di solito $x_0 = 1$.

---

### üü© **Altri pesi (j ‚â† 0):**

$$
v_j^{\text{new}} \leftarrow v_j^{\text{old}} - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} \left( h_v(x^{(i)}) - y^{(i)} \right)x_j + \frac{\lambda}{m} v_j \right]
$$

* C'√® il **termine classico del gradiente** (primo termine della somma).
* E in pi√π un **termine di regolarizzazione** $\frac{\lambda}{m} v_j$ che tende a "restringere" i pesi verso zero.
* $\lambda$ √® il coefficiente di regolarizzazione (pi√π √® grande, pi√π i pesi vengono penalizzati).

---

## üì¶ Funzione costo (non mostrata ma implicita):

Questa derivata deriva da una **loss con regolarizzazione**:

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) + \frac{\lambda}{2m} \sum_{j=1}^{d} \theta_j^2
$$

* Il secondo termine √® la **penalit√†** (regolarizzazione L2).
* La derivata rispetto a $\theta_j$ include $+ \frac{\lambda}{m} \theta_j$ (da cui il termine nell'aggiornamento).

---

## üí° Conclusione:

Questo √® un algoritmo di **gradient descent con regolarizzazione L2** (tipica della **Ridge Regression** o **logistic regression regolarizzata**).

* ‚úÖ $\theta_0$: aggiornato normalmente (senza penalizzazione).
* ‚úÖ $\theta_j$, con $j \neq 0$: aggiornato con penalizzazione.
* ‚úÖ Serve a **limitare l‚Äôoverfitting** penalizzando i pesi grandi.
* ‚úÖ La regolarizzazione √® controllata da $\lambda$.



---

## üß† **Stochastic Gradient Descent (SGD)**

### üìå **Osservazioni iniziali**

> ‚ÄúLa discesa del gradiente completa (GD) usa tutti i campioni del training set.‚Äù

‚û°Ô∏è Nel **Gradient Descent classico**, ad **ogni iterazione** bisogna calcolare il **gradiente della funzione di costo su tutti i dati**.
Questo diventa **computazionalmente costoso** per dataset molto grandi.

---

### üìâ **Problema di scalabilit√†**

> ‚ÄúLa somma dei residui coinvolge m termini ad ogni iterazione.‚Äù

* Se hai $m$ campioni, ad ogni passo devi sommare $m$ termini per calcolare il gradiente.

#### Esempio pratico:

* $m = 100.000$ campioni
* $100$ iterazioni ‚Üí ogni iterazione deve analizzare tutti i 100.000
  ‚û°Ô∏è In totale:

$$
m \times \text{iterazioni} = 100.000 \times 100 = 10.000.000 \quad \text{osservazioni processate}
$$

Se invece $m = 1.000.000$ campioni:

$$
100 \times 1.000.000 = 100.000.000 \quad \text{osservazioni!}
$$

### ‚ö†Ô∏è Questo √® molto lento!

---

## ‚ö° **Soluzione: Stochastic Gradient Descent**

> ü¶∏‚Äç‚ôÇÔ∏è **‚ÄúStochastic is super hero!‚Äù**

* Invece di usare **tutti** i campioni ad ogni iterazione, ne **campiona uno o pochi** (mini-batch) per calcolare un'approssimazione del gradiente.

### ‚ú≥Ô∏è Procedura:

#### 1. **Campiona random** un sottoinsieme dei dati

‚Äì Un piccolo **batch** di punti del training set.

#### 2. **Esegui una singola iterazione** di gradient descent

‚Äì Calcola il gradiente **solo** su quei punti campionati.

#### 3. **Aggiorna i pesi** e **ripeti** finch√© non convergi

‚Äì Continua finch√© non arrivi a una minima variazione nella funzione di costo.

> Questo metodo √® pi√π **veloce**, anche se meno preciso a ogni iterazione ‚Üí ma converge bene nel lungo termine.


---

Uno degli algoritmi fondamentali per l‚Äôottimizzazione nei modelli di machine learning √® il **Gradient Descent** (GD), che consiste nell‚Äôaggiornare iterativamente i pesi del modello nella direzione opposta al gradiente della funzione di perdita. Tuttavia, un grande svantaggio del gradient descent standard √® il **costo computazionale**, perch√© ad ogni iterazione √® necessario calcolare il gradiente **su tutto il dataset**. Questo pu√≤ diventare estremamente oneroso, soprattutto con dataset molto grandi.

Per migliorare l‚Äôefficienza, si utilizza una variante chiamata **Stochastic Gradient Descent (SGD)**. A differenza del GD classico, che usa tutti i dati contemporaneamente, lo **SGD aggiorna i pesi del modello utilizzando solo un piccolo sottoinsieme casuale dei dati**, chiamato **batch**. Il batch viene selezionato casualmente ad ogni iterazione, e la sua dimensione ‚Äì detta **batch size** ‚Äì √® di solito una potenza di due (ad esempio 32, 64 o 128). Questo approccio consente di **ridurre significativamente il carico computazionale**, pur mantenendo l‚Äôefficacia dell‚Äôottimizzazione.

Dal punto di vista del costo, se denotiamo con $m$ la dimensione del dataset, $b$ la batch size e $e$ il numero di epoche, allora il numero di aggiornamenti da fare sar√† proporzionale a $\frac{m}{b} \cdot e$. Poich√© $b \ll m$, questo significa che **SGD ha un costo molto inferiore rispetto al GD**, che richiede $m \cdot e$ aggiornamenti completi.

Un aspetto importante da considerare √® il **learning rate**, cio√® la velocit√† con cui vengono aggiornati i pesi. Esiste una relazione tra batch size e learning rate: **man mano che aumenta la batch size, √® possibile aumentare proporzionalmente anche il learning rate**. Questo perch√© batch pi√π grandi forniscono una stima del gradiente pi√π precisa e meno rumorosa. Di conseguenza, si pu√≤ fare un passo pi√π lungo nella direzione giusta, senza rischiare di divergere.

Un parametro critico √® il rapporto $\frac{\alpha}{b}$, dove $\alpha$ √® il learning rate e $b$ la batch size: **pi√π √® alto questo rapporto, maggiore √® la probabilit√† di raggiungere la convergenza** in modo stabile ed efficiente. 

---

Uno dei principali problemi del **Gradient Descent (GD)** √® legato alla scelta del **learning rate**, cio√® il passo con cui aggiorniamo i pesi del modello. L‚Äôaggiornamento dei pesi avviene secondo la formula:

$$
\theta_{\text{new}} = \theta_{\text{old}} - \alpha \cdot \nabla_\theta J(\theta)
$$

dove $\alpha$ √® il learning rate e $\nabla_\theta J(\theta)$ rappresenta il gradiente della funzione di perdita rispetto ai pesi.

Il primo problema √® che **scegliere il valore giusto del learning rate √® difficile**:

* Se √® troppo piccolo, l‚Äôalgoritmo converger√† lentamente.
* Se √® troppo grande, rischia di **saltare il minimo** della funzione di perdita e divergere.
  
La soluzione ideale sarebbe un learning rate che sia **alto quando siamo lontani dal minimo** e che **si riduca gradualmente** man mano che ci avviciniamo.

Un secondo problema √® quello di **rimanere intrappolati in un minimo locale**, specialmente in funzioni di perdita complesse. Una soluzione efficace √® usare una tecnica chiamata **momentum**.

---

### üîÅ Momentum

Il **momentum** √® un meccanismo di "memoria breve", che tiene conto della direzione degli aggiornamenti precedenti. Si introduce un parametro $\beta$ (solitamente intorno a 0.9) che pesa il contributo del passo precedente. La formula aggiornata diventa:

$$
z_{\text{new}} = \beta \cdot z_{\text{old}} + \nabla_\theta J(\theta)
$$

$$
\theta_{\text{new}} = \theta_{\text{old}} - \alpha \cdot z_{\text{new}}
$$

Questo consente al modello di **mantenere la direzione** se sta gi√† convergendo, anche quando il gradiente attuale √® piccolo o nullo. In pratica, **aiuta a uscire dai minimi locali** e a evitare oscillazioni.

---

### üìâ Adattamento del Learning Rate

Negli algoritmi pi√π moderni, non si usa un learning rate fisso. Esistono diverse tecniche che permettono di **adattarlo dinamicamente** durante l'addestramento:

* **Adagrad**: assegna **learning rate pi√π piccoli** ai parametri che ricevono aggiornamenti frequenti e **pi√π grandi** a quelli aggiornati raramente. Questo consente una **convergenza pi√π rapida** per i parametri meno influenzati e maggiore **stabilit√†** per quelli pi√π attivi.

* Questo accade perch√© alcuni pesi sono associati a **feature molto presenti nei dati**: il loro gradiente viene calcolato pi√π spesso, quindi devono essere aggiornati con cautela (learning rate piccolo).

* **Adam**: √® un algoritmo ancora pi√π sofisticato, che **combina Adagrad con il momentum**. Assegna automaticamente **learning rate diversi a ogni parametro**, tenendo conto sia della **frequenza degli aggiornamenti**, sia della **direzione passata** del gradiente.

---

### ‚úÖ In sintesi

| Problema                             | Soluzione                               |
| ------------------------------------ | --------------------------------------- |
| Learning rate difficile da scegliere | Tecniche adattive come Adagrad o Adam   |
| Rischio di rimanere in minimi locali | Momentum (con parametro $\beta$)        |
| Convergenza lenta o instabile        | Learning rate dinamico, batch, momentum |


## Osservazioni

- $b \equiv$ numero di campioni nel **mini-batch** (bellatch size)  
- $\alpha \equiv$ **learning rate**  
- $e \equiv$ numero di **epoche** (iterazioni SGD)  
- $m \equiv$ numero di **campioni**

Ad ogni epoca si aggiornano i pesi.

$$
\frac{m}{b} = \text{numero di batch da considerare per ogni epoca} \Rightarrow \lfloor\frac{m}{b}\rfloor \text{ iterazioni per epoca}
$$

$$
e \cdot \frac{m}{b} = \text{Iterazioni totali}
$$

---

- Minibatch **grandi** permettono di stimare meglio il gradiente  
  (pi√π √® grande $b$, pi√π ci si avvicina al gradiente di $J(\vec{\theta})$ sull'intero set di dati)

- Pi√π grande √® il batch, pi√π grande e **stabile** pu√≤ essere lo step da fare nel gradiente (cio√® $\alpha$)

#### Scaling:

- $b \cdot K \rightarrow \alpha \cdot K$ ‚Üí **Linear scaling** aumento il learning rate

- $b \cdot K \rightarrow \alpha \cdot \sqrt{K}$ ‚Üí **Alternativa allo scaling** per velocizzare la convergenza

---

## Model Evalutation


$$
J(\vec{\theta}) = \text{funzione ottimizzazione parametri regolarizzata (calcolata sul training)}
$$

- $J_{\text{train}}$: uguale a $J(\vec{\theta})$ ma **non** include il termine di regolarizzazione (calcolata sul training set)  
- $J_{\text{val}}$: uguale a $J(\vec{\theta})$ ma **non** include il termine di regolarizzazione (calcolata sul validation set)  
- $J_{\text{test}}$: calcolata sul test set. Uguale a $J(\vec{\theta})$ ma **non** considera il termine di regolarizzazione

---

$$
J_{\text{error}} = \frac{1}{m} \sum_{i=1}^{m} \text{Error} \left( h_{\vec{\theta}}(x^{(i)}), y^{(i)} \right)
$$

üëâ $\text{Misclassification error} =$ frazione dei pattern erroneamente classificati

---

$$
\text{Error} \left( h_{\vec{\theta}}(x^{(i)}), y^{(i)} \right) =
\begin{cases}
1 & \text{if } \left( h_{\vec{\theta}}(x^{(i)}) \geq 0.5 \land y^{(i)} = 0 \right) \lor \left( h_{\vec{\theta}}(x^{(i)}) < 0.5 \land y^{(i)} = 1 \right) \\
0 & \text{if } \left( h_{\vec{\theta}}(x^{(i)}) \geq 0.5 \land y^{(i)} = 1 \right) \lor \left( h_{\vec{\theta}}(x^{(i)}) < 0.5 \land y^{(i)} = 0 \right)
\end{cases}
$$

## Matrice di Confusione

![Confusion Matrix](media/confusionmatrix.png)



## Definizioni e Obiettivi

**Goal:**  
Vogliamo che $a$ e $b$ siano grandi, mentre $c$ e $d$ siano piccoli  
(in altre parole: massimizzare le classificazioni corrette)

---

### Significato delle variabili:

- $a$: numero di campioni di classe positiva **classificati come positivi**  
  ‚áí **Veri Positivi** ‚Üí $TP = \frac{a}{a + c}$

- $b$: numero di campioni di classe negativa **classificati correttamente come negativi**  
  ‚áí **Veri Negativi** ‚Üí $TN = \frac{b}{b + d}$

- $c$: numero di campioni positivi **classificati erroneamente come negativi**  
  ‚áí **Falsi Negativi** ‚Üí $FN = \frac{c}{a + c}$

- $d$: numero di campioni negativi **classificati erroneamente come positivi**  
  ‚áí **Falsi Positivi** ‚Üí $FP = \frac{d}{b + d}$

---

### Totali:

- $a + c =$ totale dei **campioni positivi** nella banca dati di test  
- $d + b =$ totale dei **campioni negativi** nella banca dati di test

---

### Propriet√†:

$$
TP + FN = 1
$$

$$
TN + FP = 1
$$


## Accuracy

$$
\text{Accuracy} = \frac{a+b}{a+b+c+d}
$$

Indica la percentuale dei campioni classificati correttamente.

Bisogna avere test bilanciati sul numero di campioni delle diverse classi, non √® sempre possibile ma bisogna tenerne conto in fase di valutazione.

## Classificazione Multiclasse

![Multiclass Matrix](media/multiclassmatrix.png)

## üî¢ **Matrice di confusione multi-classe**

Ogni riga rappresenta la **classe reale** di un campione, mentre ogni colonna rappresenta la **classe predetta**. La cella $M_{ij}$ contiene il numero di esempi della **classe reale $i$** classificati come **classe $j$**.

---

### üìå **Elementi chiave della matrice:**

* **$M_{ii}$**: elementi correttamente classificati della **classe $i$**
  ‚Üí chiamati anche **True Positives (TP)** per ciascuna classe

* **$M_{ij}$ con $i \ne j$**: elementi **erroneamente classificati**
  ‚Üí ad esempio, $FP_{k \rightarrow 0}$ rappresenta gli esempi **della classe $k$** erroneamente classificati come **classe $0$**

* **$FN_{0,1}$**: esempi della **classe 0** erroneamente classificati come **classe 1**

---

## ‚úÖ **Accuratezza (Accuracy)**

L'accuracy nel caso multi-classe √® definita come:

$$
\text{Accuracy} = \frac{\sum_i M_{ii}}{\sum_i \sum_j M_{ij}}
$$

### Interpretazione:

* **Numeratore**: somma di tutte le classificazioni corrette (diagonale principale)
* **Denominatore**: totale degli esempi classificati (somma di tutti gli elementi nella matrice)

---

## üß† Estensione concettuale:

L'obiettivo rimane **massimizzare gli elementi sulla diagonale** (cio√® le classificazioni corrette per ogni classe), e **minimizzare quelli fuori diagonale**, che rappresentano errori di classificazione.

Questo schema √® utile per calcolare anche **precision**, **recall**, **F1-score** per ogni classe separatamente, se necessario.




---

## üéØ **Precision**

> Misura la proporzione di esempi classificati come **positivi** che sono effettivamente **positivi**.

$$
\text{Precision}_{\text{classe}} = \frac{\text{True Positive}_{\text{classe}}}{\text{True Positive}_{\text{classe}} + \text{False Positive}_{\text{classe}}}
$$

> üîç Pi√π √® alta, meno **falsi allarmi** (falsi positivi) ci sono.

---

## üîÅ **Recall (Sensibilit√†)**

> Misura la proporzione di esempi **effettivamente positivi** che sono stati **correttamente identificati** come tali.

$$
\text{Recall}_{\text{classe}} = \frac{\text{True Positive}_{\text{classe}}}{\text{True Positive}_{\text{classe}} + \text{False Negative}_{\text{classe}}}
$$

> üîç Pi√π √® alta, meno **positivi sfuggiti** (falsi negativi) ci sono.

---

## ‚öñÔ∏è **F1-Score**

> Rappresenta la **media armonica** tra **precision** e **recall**, utile quando le due sono sbilanciate.

$$
F_1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
$$

> üîç Utile quando l'equilibrio tra falsi positivi e falsi negativi √® importante.

---

## ‚úÖ **Accuracy**

> Percentuale di **predizioni corrette** sul totale dei campioni.

$$
\text{Accuracy}(Y, \hat{Y}) = \frac{|\{ i : y^{(i)} = \hat{y}^{(i)} \}|}{|Y|}
$$

> üìâ Non √® affidabile quando le **classi sono sbilanciate** (es. malattie rare): pu√≤ essere alta anche se il modello non identifica **nessun positivo**.

---

## üìâ **Curva Precision-Recall**

Varia il **threshold (soglia di decisione)** da 0 a 1, e osserva come cambiano precision e recall. La curva mostra il trade-off tra i due:

* Quando la **soglia √® alta**, il modello √® pi√π cauto ‚Üí pi√π **precision**, meno **recall**
* Quando la **soglia √® bassa**, il modello √® pi√π permissivo ‚Üí pi√π **recall**, meno **precision**

> üîç **Un buon modello** avr√† una curva vicino all'angolo in alto a destra.

---

## üîÑ **Trade-off tra Precision e Recall**

* Se aumenti **recall**, rischi pi√π **falsi positivi** ‚Üí precision cala
* Se aumenti **precision**, rischi pi√π **falsi negativi** ‚Üí recall cala

---

