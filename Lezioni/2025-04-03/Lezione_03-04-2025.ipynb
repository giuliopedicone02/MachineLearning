{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lezione 3 aprile 2025\n",
    "\n",
    "Valutazione di un regressore lineare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenzovillanova/miniconda3/envs/ML_def/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import torch\n",
    "\n",
    "boston = fetch_openml(name=\"boston\", version=1)\n",
    "\n",
    "X = boston.data.to_numpy().astype(float)\n",
    "Y = boston.target.to_numpy().astype(float)\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "X = X[idx]\n",
    "Y = Y[idx]\n",
    "\n",
    "X_training = torch.Tensor(X[50:])\n",
    "Y_training = torch.Tensor(Y[50:])\n",
    "\n",
    "X_testing = torch.Tensor(X[:50])\n",
    "Y_testing = torch.Tensor(Y[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"Costruisce un regressore lineare.\n",
    "        Input:\n",
    "            in_size: numero di feature in input (es. 13)\n",
    "            out_size: numero di elementi in output (es. 1)\"\"\"\n",
    "        super(LinearRegressor, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        result = self.linear(x)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 300\n",
    "\n",
    "writer = SummaryWriter(\"logs/linear_regressor_1\")\n",
    "\n",
    "# normalizzazione dei dati\n",
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "\n",
    "X_training_norm = (X_training - means) / stds\n",
    "X_testing_norm = (X_testing - means) / stds\n",
    "\n",
    "reg = LinearRegressor(13, 1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(reg.parameters(), lr=lr)\n",
    "\n",
    "for e in range(epochs):\n",
    "    reg.train()\n",
    "    output = reg(X_training_norm)\n",
    "    l = criterion(output.view(-1), Y_training)\n",
    "\n",
    "    writer.add_scalar(\"loss/train\", l.item(), global_step=e)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    reg.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        y_test = reg(X_testing_norm)\n",
    "        l = criterion(y_test.view(-1), Y_testing)\n",
    "        writer.add_scalar(\"loss/test\", l.item(), global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per prevenire le etichette sia per il training sia per il testset facciamo :\n",
    "\n",
    "preds_training = reg(X_training_norm)\n",
    "preds_testing = reg(X_testing_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Errore (MSE) di training 20.52\n",
      "Mean Squared Errore (MSE) di training 44.04\n"
     ]
    }
   ],
   "source": [
    "# Un modo per valutare il modello consiste nel calcolare MSE. Definiamo una funzione e usiamola per calcolare l'errore di test:\n",
    "\n",
    "\n",
    "def MSE(predictions, gt):\n",
    "    assert predictions.shape == gt.shape\n",
    "    return ((predictions - gt) ** 2).mean()\n",
    "\n",
    "\n",
    "# Quando chiamiamo MSE, facciamo reshape delle predizioni in modo tale da passare da [N x 1] a dimensione  di shape [1]\n",
    "print(\n",
    "    \"Mean Squared Errore (MSE) di training {:0.2f}\".format(\n",
    "        MSE(preds_training.view(-1), Y_training)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Mean Squared Errore (MSE) di training {:0.2f}\".format(\n",
    "        MSE(preds_testing.view(-1), Y_testing)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Errore (RMSE) di training 4.53\n",
      "Root Mean Squared Errore (RMSE) di training 6.64\n"
     ]
    }
   ],
   "source": [
    "# Per eliminare gli errori al quadrato dell'MSE usiamo RMSE\n",
    "\n",
    "\n",
    "def RMSE(predictions, gt):\n",
    "    assert predictions.shape == gt.shape\n",
    "    return ((predictions - gt) ** 2).mean() ** (0.5)\n",
    "\n",
    "\n",
    "# Quando chiamiamo MSE, facciamo reshape delle predizioni in modo tale da passare da [N x 1] a dimensione  di shape [1]\n",
    "print(\n",
    "    \"Root Mean Squared Errore (RMSE) di training {:0.2f}\".format(\n",
    "        RMSE(preds_training.view(-1), Y_training)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Root Mean Squared Errore (RMSE) di training {:0.2f}\".format(\n",
    "        RMSE(preds_testing.view(-1), Y_testing)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) di training 3.08\n",
      "Mean Absolute Error (MAE) di training 4.31\n"
     ]
    }
   ],
   "source": [
    "# Un'altra misura di errore utilizzata Ã¨ la Mean Absolute Error (MAE)\n",
    "# che consiste nel calcolare le medie dei valori assoluti delle differenze tra valori predetti e valori di ground truth\n",
    "\n",
    "\n",
    "def MAE(predictions, gt):\n",
    "    assert predictions.shape == gt.shape\n",
    "    return ((predictions - gt).abs()).mean()\n",
    "\n",
    "\n",
    "# Quando chiamiamo MSE, facciamo reshape delle predizioni in modo tale da passare da [N x 1] a dimensione  di shape [1]\n",
    "print(\n",
    "    \"Mean Absolute Error (MAE) di training {:0.2f}\".format(\n",
    "        MAE(preds_training.view(-1), Y_training)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Mean Absolute Error (MAE) di training {:0.2f}\".format(\n",
    "        MAE(preds_testing.view(-1), Y_testing)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REC Regression Error Curve\n",
    "\n",
    "# Asse x: serie di solgie di tolleranza\n",
    "# Asse y: percentuale degli elementi di test che presentano un errore inferiore o uguale all'errore corrispndente\n",
    "# Area Over the Curve offre una misura dell'errore del metodo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def REC_curve(predictions, gt):\n",
    "    assert predictions.shape == gt.shape\n",
    "    # Usiamo la Mean Absolute Error (MAE)\n",
    "    errors = np.abs((predictions.detach().cpu().numpy()) - gt.cpu().numpy())\n",
    "    # prendiamo i valori unici degli errori ed ordiniamoli\n",
    "    tollerances = sorted(np.unique(errors))\n",
    "    correct = []\n",
    "\n",
    "    for t in tollerances:\n",
    "        correct.append(\n",
    "            (errors <= t).mean()\n",
    "        )  # Frazione degli elementi correttamente regressi\n",
    "\n",
    "    AUC = np.trapz(\n",
    "        correct, tollerances\n",
    "    )  # Area sotto la curva calcolata con il metodo dei trapezi\n",
    "    tot_area = np.max(tollerances) * 1  # Area totale\n",
    "    AOC = tot_area - AUC\n",
    "\n",
    "    return tollerances, correct, AOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiegazione del codice\n",
    "\n",
    "# ðŸ“ˆ REC Regression Error Curve â€“ Spiegazione del Codice\n",
    "\n",
    "```python\n",
    "def REC_curve(predictions, gt):\n",
    "```\n",
    "\n",
    "Definisce una funzione che costruisce una **Regression Error Curve (REC)**, utile per valutare le prestazioni di un modello di **regressione**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Cosa fa il codice?\n",
    "\n",
    "### 1. **Controllo della forma**\n",
    "```python\n",
    "assert predictions.shape == gt.shape\n",
    "```\n",
    "Verifica che `predictions` e `gt` (ground truth) abbiano la stessa forma.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Calcolo degli errori assoluti**\n",
    "```python\n",
    "errors = np.abs((predictions.detach().cpu().numpy()) - gt.cpu().numpy()) \n",
    "```\n",
    "- Converte i tensori PyTorch in array NumPy.\n",
    "- Calcola lâ€™**errore assoluto** elemento per elemento.\n",
    "\n",
    "| Funzione | Significato | Scopo |\n",
    "|----------|-------------|-------|\n",
    "| `detach()` | Stacca il tensore dal grafo dei gradienti | Serve per evitare il tracciamento dei gradienti |\n",
    "| `cpu()` | Porta il tensore sulla CPU | Necessario prima di usare `.numpy()` |\n",
    "| `numpy()` | Converte il tensore in array NumPy | Utile per usare le funzioni di NumPy |\n",
    "| `-` | Differenza tra predizione e valore reale | Calcola lâ€™errore (puÃ² essere negativo) |\n",
    "| `np.abs(...)` | Valore assoluto dellâ€™errore | Ottiene lâ€™**errore assoluto** per ogni elemento |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Tolleranze uniche ordinate**\n",
    "```python\n",
    "tollerances = sorted(np.unique(errors))\n",
    "```\n",
    "- Estrae i valori **unici** degli errori.\n",
    "- Li ordina per usarli come soglie crescenti nella curva REC.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Calcolo della frazione di errori sotto la soglia**\n",
    "```python\n",
    "correct = []\n",
    "\n",
    "for t in tollerances:\n",
    "    correct.append((errors <= t).mean())\n",
    "```\n",
    "- Per ogni soglia `t`, calcola la **percentuale** di predizioni che hanno errore â‰¤ `t`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Calcolo dellâ€™area sotto e sopra la curva**\n",
    "```python\n",
    "AUC = np.trapezoid(correct, tollerances)\n",
    "tot_area = np.max(tollerances) * 1\n",
    "AOC = tot_area - AUC\n",
    "```\n",
    "- `AUC`: area sotto la curva, misura della **precisione del modello**.\n",
    "- `AOC`: area sopra la curva, misura dellâ€™**errore del modello**. PiÃ¹ Ã¨ piccola, meglio Ã¨.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Output della funzione\n",
    "\n",
    "```python\n",
    "return tollerances, correct, AOC\n",
    "```\n",
    "\n",
    "Restituisce:\n",
    "- `tollerances`: soglie di tolleranza usate nellâ€™analisi.\n",
    "- `correct`: percentuali di predizioni corrette per ciascuna soglia.\n",
    "- `AOC`: **Area Over the Curve**, misura complessiva dellâ€™errore.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Interpretazione della REC Curve\n",
    "\n",
    "- **Asse x**: soglie di tolleranza (errore massimo accettabile).\n",
    "- **Asse y**: percentuale di predizioni con errore â‰¤ soglia.\n",
    "- Una curva che cresce piÃ¹ rapidamente indica un modello piÃ¹ accurato.\n",
    "- Una **AOC piÃ¹ bassa** indica migliori prestazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boston_linear_regression_rec = REC_curve(preds_testing.view(-1), Y_testing)\n",
    "plt.plot(boston_linear_regression_rec[0], boston_linear_regression_rec[1])\n",
    "plt.legend([\"Boston Linear Regressor. AOC: %0.2f\" % boston_linear_regression_rec[2]])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boston_linear_regression_training_rec = REC_curve(preds_training.view(-1), Y_training)\n",
    "boston_linear_regression_testing_rec = REC_curve(preds_testing.view(-1), Y_testing)\n",
    "\n",
    "plt.plot(\n",
    "    boston_linear_regression_training_rec[0], boston_linear_regression_training_rec[1]\n",
    ")\n",
    "plt.plot(\n",
    "    boston_linear_regression_testing_rec[0], boston_linear_regression_testing_rec[1]\n",
    ")\n",
    "plt.legend(\n",
    "    [\n",
    "        \"Boston [train]. AOC: %0.2f\" % boston_linear_regression_training_rec[2],\n",
    "        \"Boston [test]. AOC: %0.2f\" % boston_linear_regression_testing_rec[2],\n",
    "    ]\n",
    ")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione Logistica in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0]\n",
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Y[:100])\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Contesto\n",
    "Hai un dataset che contiene 569 esempi. Ogni esempio rappresenta una misurazione di un tessuto mammario con 30 caratteristiche (attributi) numerici che descrivono le proprietÃ  del nucleo delle cellule. Ogni esempio Ã¨ etichettato come:\n",
    "- 1 â†’ Il tumore Ã¨ maligno (cancro presente)\n",
    "- 0 â†’ Il tumore Ã¨ benigno (cancro assente)\n",
    "\n",
    "### Obiettivo\n",
    "Addestrare un modello che, dato un nuovo esempio, possa prevedere la probabilitÃ  che il tumore sia maligno â†’ cioÃ¨ la probabilitÃ  che l'etichetta sia 1.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Problema con la regressione lineare\n",
    "\n",
    "In una regressione lineare, si calcola un output cosÃ¬:\n",
    "\n",
    "$$\n",
    "z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "dove:\n",
    "- $ x_1, x_2, ..., x_n $ sono le caratteristiche dellâ€™esempio\n",
    "- $ \\theta_0, ..., \\theta_n $ sono i parametri (pesi) che il modello impara\n",
    "\n",
    "Ma la regressione lineare produce un valore $ z $ che puÃ² essere qualunque numero reale, anche negativo o maggiore di 1. Questo Ã¨ un problema, perchÃ© noi vogliamo stimare una probabilitÃ , cioÃ¨ un numero compreso tra 0 e 1.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Soluzione: Regressione Logistica\n",
    "\n",
    "La regressione logistica si basa su due concetti fondamentali:\n",
    "\n",
    "### 1. Funzione logit\n",
    "La funzione logit serve per trasformare una probabilitÃ  $ p $ in un numero reale:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p) = \\ln\\left( \\frac{p}{1 - p} \\right)\n",
    "$$\n",
    "\n",
    "> Questa funzione prende un numero tra 0 e 1 e lo trasforma in un numero che puÃ² andare da $ -\\infty $ a $ +\\infty $\n",
    "\n",
    "### 2. Modello\n",
    "In regressione logistica, si assume che:\n",
    "\n",
    "$$\n",
    "z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "sia uguale alla logit della probabilitÃ :\n",
    "\n",
    "$$\n",
    "z = \\text{logit}(P(y = 1 | x))\n",
    "$$\n",
    "\n",
    "Ma noi vogliamo la probabilitÃ , non la logit. Allora invertiamo la funzione logit â†’ otteniamo la funzione sigmoide (o logistica):\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ€ Funzione logistica (sigmoide)\n",
    "\n",
    "La funzione sigmoide Ã¨ la funzione inversa del logit:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Questa funzione ha 3 proprietÃ  fondamentali:\n",
    "- Trasforma un numero reale in un numero tra 0 e 1\n",
    "- Se $ z $ Ã¨ molto positivo, $ \\sigma(z) \\approx 1 $\n",
    "- Se $ z $ Ã¨ molto negativo, $ \\sigma(z) \\approx 0 $\n",
    "\n",
    "### Quindi:\n",
    "$$\n",
    "P(y = 1 | x) = \\sigma(z) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Interpretazione del modello\n",
    "\n",
    "Il modello dice: â€œdato un input $ x $, calcolo uno score $ z $, e poi lo trasformo in una probabilitÃ  che $ y = 1 $ usando la sigmoideâ€.\n",
    "\n",
    "---\n",
    "\n",
    "## âŒ Funzione di Loss (errore)\n",
    "\n",
    "Per addestrare il modello, abbiamo bisogno di sapere quanto sbaglia.\n",
    "\n",
    "Usiamo la log loss (o binary cross-entropy) come funzione di errore.\n",
    "\n",
    "Per un singolo esempio:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(z, y) = -y \\log(\\sigma(z)) - (1 - y) \\log(1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "> Dove:\n",
    "- $ y \\in \\{0, 1\\} $ Ã¨ la vera etichetta\n",
    "- $ \\sigma(z) $ Ã¨ la probabilitÃ  predetta\n",
    "\n",
    "### Spiegazione:\n",
    "- Se $ y = 1 $, la loss diventa $ -\\log(\\sigma(z)) $ â†’ punisce fortemente se la probabilitÃ  predetta Ã¨ bassa.\n",
    "- Se $ y = 0 $, la loss diventa $ -\\log(1 - \\sigma(z)) $ â†’ punisce fortemente se la probabilitÃ  predetta Ã¨ alta.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Loss totale (su tutto il dataset)\n",
    "\n",
    "Per tutti gli $ N $ esempi del dataset, la loss media Ã¨:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_N(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ -y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Addestramento del modello\n",
    "\n",
    "Lâ€™addestramento consiste nel trovare i valori ottimali di $ \\theta_0, \\theta_1, ..., \\theta_n $ che minimizzano la loss totale. Questo si fa con algoritmi come la discesa del gradiente.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Riassunto finale\n",
    "\n",
    "\n",
    "| Passaggio | Spiegazione |\n",
    "|----------|-------------|\n",
    "| 1. Input | Un vettore $ x $ con 30 caratteristiche |\n",
    "| 2. Score | Calcolo $ z = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n $ |\n",
    "| 3. ProbabilitÃ  | Calcolo $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ |\n",
    "| 4. Predizione | Se $ \\sigma(z) > 0.5 $, prevedo 1; altrimenti 0 |\n",
    "| 5. Errore | Uso la log-loss per calcolare quanto sbaglio |\n",
    "| 6. Allenamento | Modifico $ \\theta $ per minimizzare la loss |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108ec6870>"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "X = X[idx]\n",
    "Y = Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suddividiamo il dataset in training e testing\n",
    "X_training = torch.Tensor(X[100:])\n",
    "Y_training = torch.Tensor(Y[100:])\n",
    "\n",
    "X_testing = torch.Tensor(X[:100])\n",
    "Y_testing = torch.Tensor(Y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizziamo i dati\n",
    "\n",
    "X_mean = X_training.mean(0)\n",
    "X_std = X_training.std(0)\n",
    "\n",
    "X_training_norm = (X_training - X_mean) / X_std\n",
    "X_testing_norm = (X_testing - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor = nn.Linear(30, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1096],\n",
      "        [ 0.0380],\n",
      "        [ 0.2178],\n",
      "        [ 0.3410],\n",
      "        [ 0.1903],\n",
      "        [ 0.4142],\n",
      "        [-0.0980],\n",
      "        [-0.0238],\n",
      "        [ 0.1587],\n",
      "        [ 0.1500]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = linear_regressor(X_training_norm)\n",
    "print(z[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9687, grad_fn=<MinBackward1>)\n",
      "tensor(1.2384, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(z.min())\n",
    "print(z.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1.0 / (1 + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4726],\n",
      "        [0.5095],\n",
      "        [0.5542],\n",
      "        [0.5844],\n",
      "        [0.5474],\n",
      "        [0.6021],\n",
      "        [0.4755],\n",
      "        [0.4941],\n",
      "        [0.5396],\n",
      "        [0.5374]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "p = logistic(z)\n",
    "print(p[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2751, grad_fn=<MinBackward1>)\n",
      "tensor(0.7753, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(p.min())\n",
    "print(p.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modo alternativo offerto da Torch, si comporta allo stesso modo della funzione logistica implementata da noi\n",
    "\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2751, grad_fn=<MinBackward1>)\n",
      "tensor(0.7753, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "p2 = sigmoid(z)\n",
    "print(p2.min())\n",
    "print(p2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(p, y):\n",
    "    return (-y * torch.log(p) - (1 - y) * torch.log(1 - p)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6579, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss(p, Y_training.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa alla funzione di loss che abbiamo creato noi\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6579, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss(p, Y_training.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso abbiamo tutti gli ingredienti che ci servono per allenare il regressore logistico. Effettuiamo il training utilizzando lo schema visto nel caso della regressione lineare:\n",
    "\n",
    "1. Normalizzare i dati in ingresso $x$;\n",
    "2. Inizializzare i parametri $\\theta$ in maniera opportuna;\n",
    "3. Calcolare i logit $\\hat{z} = \\sum_i (\\theta_i x_i) + \\theta_0$;\n",
    "4. Calcolare le probabilitÃ  $p = \\frac{1}{1 + e^{-\\hat{z}}}$;\n",
    "5. Calcolare il valore della loss $\\mathcal{L}_\\theta(p, y)$;\n",
    "6. Calcolare il gradiente rispetto ai parametri $\\theta$ della funzione di loss $\\nabla_\\theta \\mathcal{L}_\\theta(p, y)$;\n",
    "7. Aggiornare i pesi $\\theta$ secondo la regola: $\\theta = \\theta - \\eta \\nabla_\\theta \\mathcal{L}_\\theta(p, y)$, dove $\\eta$ Ã¨ il learning rate;\n",
    "8. Ripetere i passi 3â€“7 fino a convergenza.\n",
    "\n",
    "\n",
    "# Domanda 1 \n",
    "\n",
    "Quali sono le differenze tra questa procedura di training e quella vista nel caso della regressione lineare?\n",
    "\n",
    "## Differenze tra regressione logistica e regressione lineare\n",
    "\n",
    "### âœ… Somiglianze\n",
    "1. **Preprocessing**:\n",
    "   - Entrambe normalizzano i dati in ingresso $ x $.\n",
    "2. **Parametri**:\n",
    "   - Entrambe usano parametri $ \\theta $ da apprendere e li inizializzano.\n",
    "3. **Struttura del modello**:\n",
    "   - Entrambe calcolano una combinazione lineare dei dati di input:  \n",
    "     $ \\hat{z} = \\sum_i \\theta_i x_i + \\theta_0 $.\n",
    "4. **Ottimizzazione**:\n",
    "   - Entrambe usano **discesa del gradiente** per aggiornare i parametri.\n",
    "5. **Loop di apprendimento**:\n",
    "   - Entrambe ripetono il calcolo della loss e lâ€™aggiornamento dei parametri fino a convergenza.\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ Differenze principali\n",
    "\n",
    "| Aspetto                | Regressione Lineare                                       | Regressione Logistica                                               |\n",
    "|------------------------|-----------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **Obiettivo**          | Prevedere un valore continuo $ \\hat{y} \\in \\mathbb{R} $ | Prevedere una probabilitÃ  $ p \\in [0, 1] $                         |\n",
    "| **Funzione di output** | Nessuna (output = $ \\hat{z} $)                          | Funzione sigmoide: $ p = \\frac{1}{1 + e^{-\\hat{z}}} $              |\n",
    "| **Loss Function**      | **MSE**: $ \\mathcal{L} = \\frac{1}{2}(y - \\hat{y})^2 $   | **Cross-Entropy**: $ \\mathcal{L} = -[y \\log p + (1 - y) \\log(1 - p)] $ |\n",
    "| **Tipo di problema**   | Regressione (output continuo)                             | Classificazione binaria (output discreto/probabilitÃ )               |\n",
    "| **Interpretazione**    | Output numerico diretto                                   | Output come probabilitÃ  della classe 1                               |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” In sintesi\n",
    "\n",
    "La **regressione lineare** si usa per stimare valori numerici, mentre la **regressione logistica** per stimare **probabilitÃ  di classi binarie**. Anche se la struttura dellâ€™algoritmo Ã¨ simile, cambiano:\n",
    "\n",
    "- lâ€™attivazione finale (sigmoide vs. identitÃ ),\n",
    "- la funzione di loss (cross-entropy vs. MSE),\n",
    "- e lâ€™interpretazione dellâ€™output (probabilitÃ  vs. valore continuo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        \"\"\"\n",
    "        Costruttore del modello di regressione logistica.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Numero di feature in input (dimensione del vettore x).\n",
    "        \"\"\"\n",
    "        # Richiamo al costruttore della classe base nn.Module\n",
    "        # Questo Ã¨ fondamentale per abilitare tutti i meccanismi interni di PyTorch,\n",
    "        # come il tracciamento dei parametri e la costruzione del grafo computazionale.\n",
    "        super(LogisticRegressor, self).__init__()\n",
    "\n",
    "        # Definizione del layer lineare:\n",
    "        # Si tratta di una trasformazione affine: y = xW^T + b\n",
    "        # Output di dimensione 1, perchÃ© la regressione logistica restituisce una sola probabilitÃ .\n",
    "        self.linear = nn.Linear(in_features, 1)\n",
    "\n",
    "        # Funzione di attivazione sigmoide, che trasforma i logit in probabilitÃ  [0,1]\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passaggio in avanti (forward pass) del modello.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Batch di input (batch_size x in_features)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Batch di probabilitÃ  (batch_size x 1)\n",
    "        \"\"\"\n",
    "        # Calcolo dei logit (valori grezzi prima della sigmoide)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        # Applicazione della funzione sigmoide ai logit per ottenere le probabilitÃ \n",
    "        return self.logistic(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Inizializza un oggetto SummaryWriter per registrare i log per TensorBoard\n",
    "writer = SummaryWriter(\"logs/logistic_regressor\")\n",
    "\n",
    "# Imposta il learning rate e il numero di epoche di training\n",
    "lr = 0.01\n",
    "epochs = 500\n",
    "\n",
    "# -----------------------------\n",
    "# Passo 1: normalizzazione dei dati\n",
    "# -----------------------------\n",
    "\n",
    "# Calcoliamo media e deviazione standard delle feature sul training set\n",
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "\n",
    "# Applichiamo la normalizzazione z-score (standardizzazione)\n",
    "X_training_norm = (X_training - means) / stds\n",
    "X_testing_norm = (X_testing - means) / stds\n",
    "# âš ï¸ Importante: usiamo media e std del training anche sul test per coerenza\n",
    "\n",
    "# -----------------------------\n",
    "# Passo 2: inizializzazione del modello\n",
    "# -----------------------------\n",
    "\n",
    "# Creiamo un'istanza del nostro regressore logistico\n",
    "# Supponiamo che il dataset abbia 30 feature in input\n",
    "regressor = LogisticRegressor(30)\n",
    "\n",
    "# Definiamo la funzione di loss: Binary Cross Entropy (per classificazione binaria)\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Definiamo lâ€™ottimizzatore: Stochastic Gradient Descent\n",
    "sgd = torch.optim.SGD(regressor.parameters(), lr)\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "for e in range(epochs):\n",
    "    regressor.train()  # Imposta il modello in modalitÃ  training (abilita dropout, batchnorm, ecc.)\n",
    "\n",
    "    # Passo 3 & 4: calcoliamo i logit + probabilitÃ  tramite il modello\n",
    "    p = regressor(X_training_norm)\n",
    "\n",
    "    # Passo 5: calcoliamo il valore della loss rispetto ai target (ground truth)\n",
    "    l = loss(p, Y_training.view(-1, 1))  # View serve a rendere Y compatibile con le dimensioni di output\n",
    "\n",
    "    # Passo 6: calcolo del gradiente della loss rispetto a tutti i parametri\n",
    "    l.backward()\n",
    "\n",
    "    # Log del valore della loss per TensorBoard (training loss)\n",
    "    writer.add_scalar('loss/train', l.item(), global_step=e)\n",
    "\n",
    "    # Passo 7: aggiornamento dei pesi tramite SGD\n",
    "    sgd.step()\n",
    "\n",
    "    # Azzeriamo i gradienti per evitare l'accumulo nel prossimo step\n",
    "    sgd.zero_grad()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Valutazione su test set\n",
    "    # -----------------------------\n",
    "    regressor.eval()  # Imposta il modello in modalitÃ  evaluation (disabilita dropout, batchnorm, ecc.)\n",
    "    with torch.set_grad_enabled(False):  # Disabilita il tracciamento dei gradienti (piÃ¹ efficiente)\n",
    "        p = regressor(X_testing_norm)\n",
    "        l = loss(p, Y_testing.view(-1, 1))  # Calcoliamo la loss anche sul test set\n",
    "        writer.add_scalar('loss/test', l.item(), global_step=e)  # Log della test loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 2\n",
    "\n",
    "Si evidenzino le differenze tra il codice mostrato e quello viato nel caso del regressore lineare\n",
    "\n",
    "| **Aspetto**                  | **Regressione Logistica**                                                                | **Regressione Lineare**                                                         |\n",
    "| ---------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| **Architettura del modello** | Usa `Sigmoid` per trasformare lâ€™output in probabilitÃ :<br>`self.logistic = nn.Sigmoid()` | Nessuna funzione di attivazione finale (output Ã¨ continuo, non una probabilitÃ ) |\n",
    "| **Funzione di loss**         | `nn.BCELoss()` (Binary Cross Entropy), adatta alla classificazione binaria               | `nn.MSELoss()` (Mean Squared Error), adatta alla regressione continua           |\n",
    "| **Interpretazione output**   | `p = regressor(x)` dÃ  **probabilitÃ ** di appartenere alla classe 1                       | `y_pred = regressor(x)` dÃ  **valori numerici continui**                         |\n",
    "| **Target (Y)**               | I target devono essere **binari** (es. 0 o 1)                                            | I target sono **numeri reali** (es. valori continui)                            |\n",
    "| **Shape del target**         | `Y.view(-1,1)` per allineare con output sigmoide `[batch_size, 1]`                       | Spesso non serve reshape, oppure si fa `Y.view(-1, 1)` per coerenza             |\n",
    "| **Uso del modello**          | Classificazione: soglia a 0.5 su output per decidere la classe (`p > 0.5 â†’ classe 1`)    | Regressione: si interpreta direttamente il valore come stima del target         |\n",
    "| **Valutazione del modello**  | PuÃ² includere **accuracy, precision, recall**, oltre alla loss                           | Si valutano **errori continui** (es. MSE, RMSE, MAE)                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1860, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Iniziamo calcolando le predizioni del modelo dati i pesi allenati\n",
    "\n",
    "p_test = regressor(X_testing_norm)\n",
    "# Calcoliamo il valore della loss\n",
    "print(loss(p_test, Y_testing.view(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False]])\n"
     ]
    }
   ],
   "source": [
    "prob_training = regressor(X_training_norm)\n",
    "prob_testing = regressor(X_testing_norm)\n",
    "\n",
    "pred_training = prob_training>=0.5\n",
    "pred_testing = prob_testing>=0.5\n",
    "\n",
    "print(pred_testing[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, gt):\n",
    "    \"\"\"\n",
    "    Calcola l'accuracy date e predizioni pred e le etichette di ground truth gt\n",
    "    \"\"\"\n",
    "    correct = pred.view(-1)==gt.view(-1)\n",
    "    \n",
    "    return float(correct.sum())/len(correct) # Conta il numero di predizioni corrette e divide per il numero totale di predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy di training: 0.97\n",
      "Accuracy di test: 0.93\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy di training: {:0.2f}\".format(accuracy(pred_training, Y_training)))\n",
    "print(\"Accuracy di test: {:0.2f}\".format(accuracy(pred_testing, Y_testing)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 4\n",
    "\n",
    "PerchÃ© il classificatore funziona meglio sul training set che su test set?\n",
    "\n",
    "Ottima domanda!\n",
    "\n",
    "Il motivo per cui il **classificatore funziona meglio sul training set che sul test set** Ã¨ legato a un fenomeno molto comune nel machine learning chiamato **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Spiegazione**\n",
    "\n",
    "Durante il training, il modello:\n",
    "\n",
    "* vede **ripetutamente** gli stessi dati,\n",
    "* e **ottimizza i suoi parametri** per minimizzare la loss **specificamente** su quei dati.\n",
    "\n",
    "Questo significa che:\n",
    "\n",
    "* Impara a **modellare molto bene (anche troppo)** le caratteristiche del training set,\n",
    "* Ma potrebbe non generalizzare altrettanto bene ai **dati mai visti prima** (come quelli del test set).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” **Cause comuni**\n",
    "\n",
    "1. **Modello troppo complesso** rispetto alla quantitÃ  di dati â†’ riesce a \"memorizzare\" anzichÃ© imparare.\n",
    "2. **Rumore** nei dati di training â†’ il modello impara anche pattern non utili.\n",
    "3. **Pochi dati** â†’ il modello non ha abbastanza varietÃ  per imparare a generalizzare.\n",
    "4. **Differenze di distribuzione** tra training e test set (dataset shift).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š **Effetti visibili**\n",
    "\n",
    "* **Loss di training bassa**, ma **loss di test piÃ¹ alta**.\n",
    "* **Accuracy di training alta**, ma **accuracy di test piÃ¹ bassa**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Conclusione**\n",
    "\n",
    "Il modello si adatta meglio ai dati su cui Ã¨ stato addestrato, ma puÃ² fallire nel **generalizzare**. Ãˆ per questo che funziona meglio sul training set.\n",
    "\n",
    "Vuoi qualche strategia per ridurre lâ€™overfitting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 5\n",
    "\n",
    "Un classificatore ottiene una sola varia a 0.91 su un dataset di 189 elementi. Quanti elementi sono stati classificati correttamente? Quanti elementi non sono stati classificati correttamente?\n",
    "\n",
    "Per rispondere alla domanda, usiamo il fatto che la **accuratezza (accuracy)** Ã¨ la frazione di esempi classificati correttamente:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Elementi corretti}}{\\text{Totale elementi}}\n",
    "$$\n",
    "\n",
    "### âœ… Dati:\n",
    "\n",
    "* Accuracy = 0.91\n",
    "* Totale elementi = 189\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¢ Calcolo:\n",
    "\n",
    "**Elementi classificati correttamente:**\n",
    "\n",
    "$$\n",
    "189 \\times 0.91 = 171.99 \\approx 172\n",
    "$$\n",
    "\n",
    "**Elementi classificati **non** correttamente:**\n",
    "\n",
    "$$\n",
    "189 - 172 = 17\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Risposta:\n",
    "\n",
    "* âœ… **172 elementi** sono stati classificati correttamente.\n",
    "* âŒ **17 elementi** non sono stati classificati correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Inizializza un oggetto SummaryWriter per registrare i log per TensorBoard\n",
    "writer = SummaryWriter(\"logs/logistic_regressor_2\")\n",
    "\n",
    "# Imposta il learning rate e il numero di epoche di training\n",
    "lr = 0.01\n",
    "epochs = 500\n",
    "\n",
    "# -----------------------------\n",
    "# Passo 1: normalizzazione dei dati\n",
    "# -----------------------------\n",
    "\n",
    "# Calcoliamo media e deviazione standard delle feature sul training set\n",
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "\n",
    "# Applichiamo la normalizzazione z-score (standardizzazione)\n",
    "X_training_norm = (X_training - means) / stds\n",
    "X_testing_norm = (X_testing - means) / stds\n",
    "# âš ï¸ Importante: usiamo media e std del training anche sul test per coerenza\n",
    "\n",
    "# -----------------------------\n",
    "# Passo 2: inizializzazione del modello\n",
    "# -----------------------------\n",
    "\n",
    "# Creiamo un'istanza del nostro regressore logistico\n",
    "# Supponiamo che il dataset abbia 30 feature in input\n",
    "regressor = LogisticRegressor(30)\n",
    "\n",
    "# Definiamo la funzione di loss: Binary Cross Entropy (per classificazione binaria)\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Definiamo lâ€™ottimizzatore: Stochastic Gradient Descent\n",
    "sgd = torch.optim.SGD(regressor.parameters(), lr)\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "for e in range(epochs):\n",
    "    regressor.train()  # Imposta il modello in modalitÃ  training (abilita dropout, batchnorm, ecc.)\n",
    "\n",
    "    # Passo 3 & 4: calcoliamo i logit + probabilitÃ  tramite il modello\n",
    "    p = regressor(X_training_norm)\n",
    "\n",
    "    # Passo 5: calcoliamo il valore della loss rispetto ai target (ground truth)\n",
    "    l = loss(p, Y_training.view(-1, 1))  # View serve a rendere Y compatibile con le dimensioni di output\n",
    "\n",
    "    # Passo 6: calcolo del gradiente della loss rispetto a tutti i parametri\n",
    "    l.backward()\n",
    "\n",
    "    # Log del valore della loss per TensorBoard (training loss)\n",
    "    writer.add_scalar('loss/train', l.item(), global_step=e)\n",
    "\n",
    "    # Passo 7: aggiornamento dei pesi tramite SGD\n",
    "    sgd.step()\n",
    "\n",
    "    # Azzeriamo i gradienti per evitare l'accumulo nel prossimo step\n",
    "    sgd.zero_grad()\n",
    "\n",
    "    writer.add_scalar('accuracy/train', accuracy(p>=0.5,Y_training),global_step=e)\n",
    "    # -----------------------------\n",
    "    # Valutazione su test set\n",
    "    # -----------------------------\n",
    "    regressor.eval()  # Imposta il modello in modalitÃ  evaluation (disabilita dropout, batchnorm, ecc.)\n",
    "    with torch.set_grad_enabled(False):  # Disabilita il tracciamento dei gradienti (piÃ¹ efficiente)\n",
    "        p = regressor(X_testing_norm)\n",
    "        l = loss(p, Y_testing.view(-1, 1))  # Calcoliamo la loss anche sul test set\n",
    "        writer.add_scalar('loss/test', l.item(), global_step=e)  # Log della test loss\n",
    "        writer.add_scalar('accuracy/test', accuracy(p>=0.5,Y_testing),global_step=e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 StabilitÃ  Numerica\n",
    "\n",
    "Finora abbiamo utilizzato la loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\theta(z, y) = \\frac{1}{N} \\sum_i -y_i \\log(p_i) - (1 - y_i) \\log(1 - p_i) \\tag{11}\n",
    "$$\n",
    "\n",
    "dove:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{1}{1 + e^{-z_i}} \\tag{12}\n",
    "$$\n",
    "\n",
    "Questa loss puÃ² essere numericamente instabile in quanto, per valori di $z_i$ molto bassi, $e^{-z_i}$ sarÃ  un numero molto alto e di conseguenza $p_i$ sarÃ  molto piccolo. Se il valore di $p_i$ scende al di sotto della precisione della macchina (cioÃ¨ va in underflow), esso verrÃ  arrotondato a zero. A questo punto, all'interno della funzione di loss $\\mathcal{L}$, il valore $y_i \\log(p_i)$ restituirÃ  un nan se $y_i = 1$.\n",
    "\n",
    "Per evitare questi problemi, in genere si utilizza la loss definita a partire dai logit:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\theta(z, y) = \\frac{1}{N} \\sum_i y_i \\log(1 + e^{-z_i}) + (1 - y_i) \\log(1 + e^{z_i}) \\tag{13}\n",
    "$$\n",
    "\n",
    "Notiamo che questa loss non soffre dei problemi discussi sopra. Per valori molto bassi di $z_i$, infatti $e^{-z_i}$ non comparirÃ , mentre il termine $1 + e^{-z_i}$ sarÃ  almeno pari a 1 (per cui potremo sempre calcolarne il logaritmo). Possiamo dunque effettuare la procedura di training evitando di calcolare esplicitamente le probabilitÃ  $p_i$ e applicando la seconda loss direttamente ai logit $z_i$. Definiamo la loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La cross-entropy per un singolo esempio Ã¨:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p, y) = -[y \\log(p) + (1 - y) \\log(1 - p)]\n",
    "$$\n",
    "\n",
    "* Il termine $\\log(p)$ Ã¨ negativo se $p < 1$\n",
    "* Il termine $\\log(1 - p)$ Ã¨ negativo se $p > 0$\n",
    "\n",
    "### Senza il segno meno:\n",
    "\n",
    "La loss potrebbe risultare negativa, e noi vogliamo invece minimizzarla, non massimizzare qualcosa di negativo.\n",
    "\n",
    "> Il segno meno trasforma quindi la log-probabilitÃ  in una quantitÃ  da minimizzare, mantenendo il concetto: piÃ¹ il modello Ã¨ certo e corretto, piÃ¹ bassa sarÃ  la loss.\n",
    "\n",
    "In pratica:\n",
    "\n",
    "* Se la predizione Ã¨ perfetta ($p = y$), la loss sarÃ  vicina a 0\n",
    "* Se la predizione Ã¨ completamente sbagliata ($p \\ll 1$ quando $y = 1$), la loss sarÃ  grande\n",
    "\n",
    "Ti interessa anche vedere un esempio numerico per chiarire il concetto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss2(z,y):\n",
    "    return (y*torch.log(1+torch.exp(-z))+(1-y)*torch.log(1+torch.exp(z))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(LogisticRegressor, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.5596, grad_fn=<MinBackward1>) tensor(1.4102, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0718, grad_fn=<MinBackward1>) tensor(0.8038, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "regressor =LogisticRegressor(30)\n",
    "logistic = nn.Sigmoid()\n",
    "\n",
    "#logits\n",
    "z = regressor(X_training_norm)\n",
    "print(z.min(),z.max())\n",
    "\n",
    "#probabilitÃ \n",
    "p = logistic(z)\n",
    "print(p.min(),p.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8208, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.8208, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_stable = nn.BCEWithLogitsLoss()\n",
    "l_1 = loss_stable(z,Y_training.view(-1,1))\n",
    "l_2 = loss2(z,Y_training.view(-1,1))\n",
    "print(l_1)\n",
    "print(l_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La procedura di training diventa dunque la seguente:\n",
    "\n",
    "1. Normalizzare i dati in ingresso x;\n",
    "2. Inizializzare i parametri $\\theta$ in maniera opportuna;\n",
    "3. Calcolare i logit $\\hat{z} = \\sum_i (\\theta_i x_i) + \\theta_0$;\n",
    "4. Calcolare il valore della loss $\\mathcal{L}_\\theta(z, y)$;\n",
    "5. Calcolare il gradiente rispetto ai parametri $\\theta$ della funzione di loss $\\nabla_\\theta \\mathcal{L}_\\theta(z, y)$;\n",
    "6. Aggiornare i pesi $\\theta$ secondo la regola: $\\theta = \\theta - \\eta \\nabla_\\theta \\mathcal{L}_\\theta(z, y)$, dove $\\eta$ Ã¨ il learning rate;\n",
    "7. Ripetere i passi 3â€“6 fino a convergenza.\n",
    "\n",
    "Modifichiamo il codice precedente per effettuare il training utilizzando la nuova loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Inizializza un oggetto SummaryWriter per registrare i log per TensorBoard\n",
    "writer = SummaryWriter(\"logs/logistic_regressor_3\")\n",
    "\n",
    "# Imposta il learning rate e il numero di epoche di training\n",
    "lr = 0.01\n",
    "epochs = 500\n",
    "\n",
    "# -----------------------------\n",
    "# Passo 1: normalizzazione dei dati\n",
    "# -----------------------------\n",
    "\n",
    "# Calcoliamo media e deviazione standard delle feature sul training set\n",
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "\n",
    "# Applichiamo la normalizzazione z-score (standardizzazione)\n",
    "X_training_norm = (X_training - means) / stds\n",
    "X_testing_norm = (X_testing - means) / stds\n",
    "# âš ï¸ Importante: usiamo media e std del training anche sul test per coerenza\n",
    "\n",
    "# -----------------------------\n",
    "# Passo 2: inizializzazione del modello\n",
    "# -----------------------------\n",
    "\n",
    "# Creiamo un'istanza del nostro regressore logistico\n",
    "# Supponiamo che il dataset abbia 30 feature in input\n",
    "regressor = LogisticRegressor(30)\n",
    "\n",
    "# Definiamo la funzione di loss: Binary Cross Entropy (per classificazione binaria)\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Definiamo lâ€™ottimizzatore: Stochastic Gradient Descent\n",
    "sgd = torch.optim.SGD(regressor.parameters(), lr)\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "for e in range(epochs):\n",
    "    regressor.train()  # Imposta il modello in modalitÃ  training (abilita dropout, batchnorm, ecc.)\n",
    "\n",
    "    # Passo 3 & 4: calcoliamo i logit + probabilitÃ  tramite il modello\n",
    "    z = regressor(X_training_norm)\n",
    "\n",
    "    # Passo 5: calcoliamo il valore della loss rispetto ai target (ground truth)\n",
    "    l = loss(z, Y_training.view(-1, 1))  # View serve a rendere Y compatibile con le dimensioni di output\n",
    "\n",
    "    # Passo 6: calcolo del gradiente della loss rispetto a tutti i parametri\n",
    "    l.backward()\n",
    "\n",
    "    # Log del valore della loss per TensorBoard (training loss)\n",
    "    writer.add_scalar('loss/train', l.item(), global_step=e)\n",
    "\n",
    "    # Passo 7: aggiornamento dei pesi tramite SGD\n",
    "    sgd.step()\n",
    "\n",
    "    # Azzeriamo i gradienti per evitare l'accumulo nel prossimo step\n",
    "    sgd.zero_grad()\n",
    "\n",
    "    p=logistic(z)\n",
    "    writer.add_scalar('accuracy/train', accuracy(p>=0.5,Y_training),global_step=e)\n",
    "    # -----------------------------\n",
    "    # Valutazione su test set\n",
    "    # -----------------------------\n",
    "    regressor.eval()  # Imposta il modello in modalitÃ  evaluation (disabilita dropout, batchnorm, ecc.)\n",
    "    with torch.set_grad_enabled(False):  # Disabilita il tracciamento dei gradienti (piÃ¹ efficiente)\n",
    "        z = regressor(X_testing_norm)\n",
    "        l = loss(z, Y_testing.view(-1, 1))  # Calcoliamo la loss anche sul test set\n",
    "        writer.add_scalar('loss/test', l.item(), global_step=e)  # Log della test loss\n",
    "        p=logistic(z)\n",
    "        writer.add_scalar('accuracy/test', accuracy(p>=0.5,Y_testing),global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy di training: 0.97\n",
      "Accuracy di test: 0.93\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy di training: {:0.2f}\".format(accuracy(logistic(regressor(X_training_norm))>=0.5,Y_training)))\n",
    "print(\"Accuracy di test: {:0.2f}\".format(accuracy(logistic(regressor(X_testing_norm))>=0.5,Y_testing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_def",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
