# Venerd√¨ 11 aprile 2025

## Multi-Class Classification

![Multiclass Classification](media/multiclass.png)

* Stai considerando un caso in cui un classificatore deve distinguere tra pi√π di 2 classi (es. Class 1, Class 2, Class 3).
* Viene mostrato un esempio 2D con tre tipi di punti (triangoli, cerchi e quadrati), ciascuno appartenente a una classe diversa.


---

## üìä Probabilit√† e Decisione

Nella strategia One VS All si fa il training del classificatore binario per ogni classe k al fine i predire la probabilit√†.
### Formula:

$$
P(\omega_k | x)
$$

* √à la **probabilit√† a posteriori** della classe $\omega_k$ dato il punto $x$.

### Decision Rule (regola di decisione):

$$
R = \text{max}_{k} \, P(\omega_k | x)
$$

* Il punto $x$ viene assegnato alla **classe con probabilit√† a posteriori massima**.
* Questo √® un approccio **Bayesiano** alla classificazione.

---

## üìå Esempio

$$
\begin{align*}
h_1(x) &= 0.3 \\
h_2(x) &= 0.8 \\
h_3(x) &= 0.1
\end{align*}
\Rightarrow \text{Classificato in } \textbf{Classe 2}
$$

* Gli output $h_k(x)$ sono le uscite dei classificatori.
* Anche se nessuna probabilit√† supera 0.5, si sceglie quella **con valore massimo**.

$$
\begin{align*}
h_1(x) &= 0.4 \\
h_2(x) &= 0.2 \\
h_3(x) &= 0.1
\end{align*}
\Rightarrow \text{Classificato in } \textbf{Classe 1}
$$
üìå Nota:

$$
\sum_k h_k(x) \quad \text{pu√≤ assumere valori > 1}
$$

* Perch√© i classificatori non formano una distribuzione di probabilit√†.

---



**"E se tutti i classificatori avessero un valore di output < 0.5?"**

üëâ Risposta:
Si prende comunque il classificatore con il valore **massimo**, anche se < 0.5.
Esempio: se $h_1=0.4, h_2=0.2, h_3=0.1$ ‚Üí si sceglie Classe 1, perch√© ha score massimo.

---

## üîÄ One-vs-All

![Multiclass Classification](media/onevsall.png)


Ogni classificatore distingue tra:

* **una classe** vs **tutte le altre**.

Esempio:

* $h_1(m) < 0.5$
* $h_2(m) < 0.5$
* $h_3(m) < 0.5$

Regola:
Se $h_2(m) < h_3(m) < h_1(m) \rightarrow$  Classe 1



---

## ü§º‚Äç‚ôÇÔ∏è One-vs-One

![Multiclass Classification](media/onevsone.png)


* Si addestrano classificatori per ogni **coppia di classi**:

  $$
  \text{Numero di classificatori} = \frac{K(K-1)}{2}
  $$

  dove $K$ √® il numero di classi.

* Ogni classificatore decide tra due classi. Alla fine si vota la classe con pi√π vittorie (sistema di voto a maggioranza).

üìå Esempio:
Classi $C_1, C_2, C_3$

* $C_1$ vs $C_2$ ‚Üí vince $C_1$
* $C_2$ vs $C_3$ ‚Üí vince $C_3$
* $C_1$ vs $C_3$ ‚Üí vince $C_1$

Risultato: **Classe $C_1$**

üìç Nota: i **confini di decisione** sono pi√π precisi in questo schema, ma pi√π costosi da calcolare.

---

## üìâ Decisione binaria

![Multiclass Classification](media/binaryclass.png)


Mostra la curva di probabilit√† per due classi $C_1$ e $C_2$:

* Dove si incrociano (linea tratteggiata verde) si trova la **decision boundary**.
* Zona in mezzo: **reject region** ‚Üí valori in cui non √® possibile decidere con sicurezza.

üëâ Questo suggerisce che a volte, se nessuna classe ha abbastanza confidenza, si pu√≤ decidere di **non classificare**.

