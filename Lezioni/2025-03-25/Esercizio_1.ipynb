{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e705a77",
   "metadata": {},
   "source": [
    "Si provi ad allenare il regressore sul dataset Boston senza effettuare la normalizzazione dei dati. Se necessario si modifichino il learning rate e il numero di epoche per portare il modello a convergenza. Si notino differenze durante il training? Il modello allenato è migliore o peggiore? Si monitorino le curve di training e test mediante tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0aed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "boston = fetch_openml(name='boston', version=1) # Serve per ottenere dei datasets da openml\n",
    "\n",
    "X = boston.data.to_numpy().astype(float) # Convertiamo un dataframe in un array in modo tale da poter fare la shape\n",
    "Y = boston.target.to_numpy().astype(float) # stessa cosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "X = X[idx]\n",
    "Y = Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = torch.Tensor(X[50:])\n",
    "Y_training = torch.Tensor(Y[50:])\n",
    "\n",
    "X_testing = torch.Tensor(X[:50])\n",
    "Y_testing = torch.Tensor(Y[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.Tensor(13) # Creiamo un tensore di 13 unità\n",
    "theta_0= torch.Tensor(1) # Tensore di una unità (bias)\n",
    "\n",
    "#impostiamo required_grad\n",
    "theta.requires_grad_(True)\n",
    "theta_0.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6809cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(input,theta,theta_0):\n",
    "    return input.mul(theta).sum(1)+theta_0\n",
    "    #.mul(theta) fa un prodotto elemento per elemento tra ogni riga dell'input e theta\n",
    "    #.sum(1) Dopo il prodotto elemento per elemento, sommi lungo l’asse 1 (cioè per riga) \n",
    "    # + theta_0 Aggiungi il bias a ciascuna predizione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0bb313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(input, target):\n",
    "        return ((input-target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('logs/Esercizio1')\n",
    "\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"Costruisce un regressore logistico.\n",
    "        Input:\n",
    "            in_size: numero di feature in input (es. 13)\n",
    "            out_size: numero di elementi in output (es. 1)\"\"\"\n",
    "        super(LinearRegressor, self).__init__() #Richiamo del costruttore della superclasse\n",
    "        # Questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di Pytorch\n",
    "        self.linear = nn.Linear(in_size,out_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        result = self.linear(x)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.000001\n",
    "epochs = 1000000\n",
    "\n",
    "# Modello\n",
    "reg = LinearRegressor(13, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(reg.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for e in range(epochs):\n",
    "    reg.train()\n",
    "    output = reg(X_training)\n",
    "    l = criterion(output.view(-1), Y_training)\n",
    "    train_losses.append(l.item())\n",
    "\n",
    "    writer.add_scalar('loss/train', train_losses[-1], global_step=e)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    reg.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        y_test = reg(X_testing)\n",
    "        l = criterion(y_test.view(-1), Y_testing)\n",
    "        test_losses.append(l.item())\n",
    "                            \n",
    "        writer.add_scalar('loss/test', test_losses[-1], global_step=e)\n",
    "\n",
    "    if (e+1)%100 == 0:\n",
    "        print(f'[Epoc {e+1}]\\t train loss: {np.round(train_losses[-1], 2)}    test loss: {np.round(test_losses[-1], 2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
